import numpy as np
import matplotlib.pyplot as plt
from queue import Queue
import tensorflow as tf
import os
from tempfile import TemporaryFile
from tensorflow import keras
from keras import backend as K
from keras.utils.generic_utils import get_custom_objects
from collections import deque
from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import Adam
from keras import backend as K
import random
import math
import copy
import time


#Cp and starting positions dataset for the Env : 

cp1 = [[12937, 7226], [5628, 2585], [4111, 7409], [13477, 2311]]
p1_1 = [[12586, 7592, 214], [13114, 6764, 204]]
p2_1 = [[12074, 8438, 222], [13667, 5929, 203]]

cp2 =[[6028, 5351], [11313, 2836], [7528, 6958]]
p1_2 =[[5911, 4882, 350], [6323, 5742, 323]]
p2_2 =[[5462, 3982, 349], [6734, 6654, 320]]

cp3 =[[11310, 2804], [7501, 6943], [6008, 5346]]
p1_3 =[[11584, 3178, 160], [10860, 2522, 145]]
p2_3 =[[12346, 3863, 148], [10169, 1859, 118]]

cp4 =[[10676, 2257], [8713, 7448], [7210, 2169], [3570, 5257], [13855, 5054]]
p1_4 =[[11060, 2489, 147], [10141, 2154, 132]]
p2_4 =[[12032, 2853, 126], [9265, 1806, 96]]

cp5 =[[12954, 7224], [5614, 2556], [4109, 7414], [13484, 2347]]
p1_5 =[[12603, 7590, 214], [13130, 6762, 204]]
p2_5 =[[12090, 8436, 222], [13685, 5927, 203]]

cp6 =[[5036, 5269], [11504, 6090], [9115, 1833]]
p1_6 =[[5199, 4764, 355], [5065, 5726, 337]]
p2_6 =[[5300, 3809, 20], [4927, 6749, 354]]

cp7 =[[6274, 7776], [14100, 7730], [13904, 1239], [10229, 4937], [6092, 2185], [3017, 5177]]
p1_7 =[[6324, 7191, 302], [6319, 8185, 295]]
p2_7 =[[6344, 6291, 11], [6361, 9260, 349]]

cp8 =[[13471, 2344], [12927, 7211], [5657, 2595], [4072, 7414]]
p1_8 =[[13874, 2433, 161], [12883, 2329, 156]]
p2_8 =[[14930, 2584, 113], [11995, 2256, 79]]

cp9 =[[4039, 4664], [13069, 1882], [6532, 7842], [7467, 1387], [12719, 7119]]
p1_9 =[[3992, 4194, 4], [4285, 5125, 350]]
p2_9 =[[3676, 3219, 352], [4553, 6063, 334]]

cp10 =[[9103, 1825], [5025, 5270], [11473, 6099]]
p1_10 =[[9373, 2292, 122], [8755, 1540, 104]]
p2_10 =[[9998, 3004, 156], [8090, 745, 124]]

cp11 =[[3624, 4437], [7983, 7909], [13296, 5524], [9586, 1371]]
p1_11 =[[4035, 4057, 6], [3412, 4821, 356]]
p2_11 =[[4606, 3328, 54], [2762, 5642, 23]]

cp12 =[[6562, 7812], [7495, 1333], [12672, 7115], [4085, 4640], [13045, 1896]]
p1_12 =[[6118, 7655, 301], [7084, 7787, 286]]
p2_12 =[[5106, 7523, 291], [8040, 7946, 265]]

cp13 =[[12456, 1340], [10538, 5953], [3577, 5188], [13609, 7609]]
p1_13 =[[12832, 1584, 149], [11917, 1212, 140]]
p2_13 =[[13790, 1978, 129], [11063, 844, 96]]

cp14 =[[5975, 4257], [14631, 1438], [3480, 7219], [9415, 7256]]
p1_14 =[[5915, 3813, 18], [6229, 4720, 353]]
p2_14 =[[5590, 2819, 351], [6510, 5646, 333]]

cp15 =[[5647, 2588], [4113, 7420], [13495, 2316], [12933, 7214]]
p1_15 =[[6197, 2807, 43], [5251, 2496, 36]]
p2_15 =[[7032, 3108, 124], [4215, 2214, 91]]

cp16 =[[3611, 5281], [13856, 5066], [10663, 2266], [8684, 7431], [7230, 2145]]
p1_16 =[[3701, 4775, 356], [3717, 5753, 344]]
p2_16 =[[3659, 3791, 7], [3721, 6768, 350]]

cp17 =[[4090, 4655], [13057, 1915], [6537, 7838], [7496, 1348], [12678, 7119]]
p1_17 =[[4044, 4185, 5], [4335, 5116, 350]]
p2_17 =[[3731, 3209, 352], [4600, 6055, 334]]

cp18 =[[7232, 6660], [5447, 2811], [10346, 3353], [11208, 5403]]
p1_18 =[[6824, 6781, 297], [7702, 6351, 279]]
p2_18 =[[5863, 7211, 265], [8537, 5972, 226]]

cp19 =[[9114, 1812], [5023, 5258], [11452, 6063]]
p1_19 =[[9383, 2279, 122], [8767, 1527, 104]]
p2_19 =[[10007, 2992, 156], [8103, 731, 124]]

cp20 =[[10339, 3386], [11210, 5418], [7254, 6664], [5453, 2846]]
p1_20 =[[10708, 3231, 155], [9789, 3627, 154]]
p2_20 =[[11703, 2874, 101], [9027, 4020, 33]]

cp21 =[[13489, 2340], [12958, 7218], [5655, 2597], [4119, 7414]]
p1_21 =[[13892, 2427, 161], [12901, 2327, 156]]
p2_21 =[[14948, 2576, 113], [12013, 2257, 79]]

cp22 =[[14636, 1411], [3463, 7209], [9417, 7262], [5960, 4241]]
p1_22 =[[14773, 1891, 159], [14318, 1015, 151]]
p2_22 =[[15252, 2770, 159], [13879, 125, 146]]

cp23 =[[10259, 4928], [6108, 2181], [3038, 5194], [6298, 7730], [14085, 7750], [13857, 1231]]
p1_23 =[[9891, 5306, 203], [10435, 4511, 180]]
p2_23 =[[9380, 6117, 230], [11010, 3654, 197]]

cp24 =[[5947, 4254], [14653, 1390], [3429, 7203], [9398, 7261]]
p1_24 =[[5886, 3810, 18], [6202, 4717, 353]]
p2_24 =[[5557, 2817, 351], [6487, 5642, 332]]

cp25 =[[10678, 2287], [8728, 7475], [7193, 2170], [3570, 5253], [13827, 5105]]
p1_25 =[[11062, 2517, 147], [10142, 2184, 133]]
p2_25=[[12035, 2880, 126], [9266, 1839, 95]]

cp26 =[[9560, 1380], [3663, 4442], [8002, 7885], [13320, 5543]]
p1_26 =[[9734, 1907, 124], [9295, 1030, 110]]
p2_26 =[[10174, 2731, 165], [8808, 101, 140]]

cp27 =[[14103, 7749], [13905, 1232], [10266, 4929], [6102, 2211], [3038, 5161], [6297, 7741]]
p1_27 =[[13517, 7714, 210], [14513, 7690, 206]]
p2_27 =[[12620, 7717, 281], [15582, 7626, 255]]

cp28 =[[13321, 5558], [9572, 1375], [3670, 4436], [7999, 7926]]
p1_28 =[[12853, 5865, 196], [13594, 5211, 187]]
p2_28 =[[12168, 6488, 243], [14371, 4513, 213]]

cp29 =[[10570, 6008], [3610, 5178], [13608, 7591], [12484, 1334]]
p1_29 =[[10433, 6442, 219], [10536, 5476, 201]]
p2_29 =[[10316, 7471, 199], [10668, 4526, 175]]

cp30 =[[5966, 4233], [14640, 1428], [3425, 7246], [9401, 7264]]
p1_30 =[[5907, 3789, 19], [6219, 4698, 354]]
p2_30 =[[5583, 2794, 351], [6499, 5623, 333]]

cp31 =[[11317, 2802], [7470, 6951], [6019, 5344]]
p1_31 =[[11590, 3177, 160], [10868, 2519, 145]]
p2_31 =[[12349, 3865, 148], [10179, 1853, 118]]

cp32 =[[7180, 2149], [3613, 5257], [13849, 5079], [10697, 2272], [8708, 7462]]
p1_32 =[[7532, 2623, 76], [6891, 1864, 67]]
p2_32 =[[8092, 3312, 157], [6153, 1086, 121]]

cp33 =[[7214, 2139], [3618, 5266], [13839, 5059], [10663, 2256], [8694, 7445]]
p1_33 =[[7564, 2613, 77], [6924, 1855, 68]]
p2_33 =[[8125, 3303, 156], [6188, 1075, 122]]

cp34 =[[11455, 6073], [9086, 1858], [5001, 5286]]
p1_34 =[[10933, 6266, 211], [11796, 5796, 199]]
p2_34 =[[10130, 6730, 258], [12705, 5283, 223]]

cp35 =[[7463, 1369], [12730, 7083], [4031, 4652], [13028, 1887], [6578, 7834]]
p1_35 =[[7836, 1130, 87], [7126, 1803, 72]]
p2_35 =[[8608, 420, 58], [6424, 2433, 36]]

cp36 =[[4980, 5274], [11499, 6099], [9108, 1847]]
p1_36 =[[5143, 4769, 355], [5009, 5732, 338]]
p2_36 =[[5243, 3813, 20], [4872, 6754, 354]]

cp37 =[[3608, 5186], [13583, 7610], [12454, 1370], [10511, 5957]]
p1_37 =[[3826, 4695, 357], [3587, 5647, 345]]
p2_37 =[[4036, 3758, 22], [3334, 6651, 5]]

cp38 =[[8675, 7442], [7176, 2178], [3576, 5302], [13848, 5070], [10707, 2295]]
p1_38 =[[8188, 7479, 266], [9118, 7213, 248]]
p2_38 =[[7231, 7773, 269], [10077, 6963, 239]]

cp39 =[[10550, 5974], [3566, 5193], [13573, 7617], [12489, 1350]]
p1_39 =[[10416, 6409, 218], [10512, 5442, 201]]
p2_39 =[[10307, 7440, 198], [10637, 4491, 174]]

cp40 =[[2664, 7005], [10034, 5963], [13929, 1929], [8022, 3282]]
p1_40 =[[2688, 6475, 340], [2821, 7450, 330]]
p2_40 =[[2534, 5525, 3], [2949, 8463, 341]]

cp41 =[[3138, 7557], [9507, 4372], [14496, 7807], [6329, 4261], [7815, 838], [7638, 5975]]
p1_41 =[[3003, 7064, 333], [3442, 7944, 323]]
p2_41 =[[2544, 6195, 345], [3872, 8849, 322]]

cp42 =[[14571, 7698], [10587, 5072], [13075, 2307], [4568, 2180], [7336, 4925], [3324, 7247]]
p1_42 =[[14209, 8065, 210], [14753, 7243, 202]]
p2_42 =[[13694, 8888, 231], [15320, 6424, 196]]

cp43 =[[7974, 7871], [13318, 5559], [9535, 1409], [3646, 4411]]
p1_43 =[[7783, 7312, 274], [8168, 8230, 267]]
p2_43 =[[7457, 6482, 351], [8633, 9199, 322]]

cp44 =[[3662, 4398], [8028, 7884], [13289, 5519], [9589, 1417]]
p1_44 =[[4073, 4019, 7], [3450, 4783, 356]]
p2_44 =[[4645, 3290, 54], [2799, 5602, 24]]

cp45 =[[10050, 5997], [13946, 1951], [8011, 3284], [2650, 7004]]
p1_45 =[[9607, 5594, 214], [10331, 6283, 217]]
p2_45 =[[9037, 4916, 329], [11170, 6967, 299]]

cp46 =[[7473, 1364], [12718, 7074], [4075, 4684], [13036, 1910], [6551, 7816]]
p1_46 =[[7846, 1126, 87], [7135, 1797, 72]]
p2_46 =[[8620, 417, 58], [6432, 2427, 36]]

cp47 =[[13061, 1907], [6566, 7816], [7498, 1372], [12728, 7097], [4030, 4658]]
p1_47 =[[13305, 2315, 158], [12640, 1590, 148]]
p2_47 =[[14003, 3060, 147], [12003, 860, 128]]

cp48 =[[10061, 5944], [13897, 1934], [8033, 3240], [2691, 7009]]
p1_48 =[[9616, 5544, 213], [10342, 6231, 216]]
p2_48=[[9045, 4866, 329], [11183, 6911, 299]]

cp49 =[[3299, 7201], [14597, 7695], [10531, 5072], [13102, 2294], [4564, 2169], [7350, 4952]]
p1_49 =[[3411, 6658, 335], [3360, 7645, 326]]
p2_49 =[[3444, 5716, 10], [3313, 8693, 355]]

cp50 =[[9423, 7217], [5941, 4222], [14670, 1385], [3424, 7227]]
p1_50 =[[9064, 7502, 250], [9689, 6758, 233]]
p2_50 =[[8404, 8286, 239], [10327, 6049, 203]]

cp_data = [cp1,cp2,cp3,cp4,cp5,cp6,cp7,cp8,cp9,cp10,cp11,cp12,cp13,cp14,cp15,cp16,cp17,cp18,cp19,cp20,cp21,cp22,cp23,cp24,cp25,cp26,cp27,cp28,cp29,cp30,cp31,cp32,cp33,cp34,cp35,cp36,cp37,cp38,cp39,cp40,cp41,cp42,cp43,cp44,cp45,cp46,cp47,cp48,cp49,cp50]

p1_data = [p1_1,p1_2,p1_3,p1_4,p1_5,p1_6,p1_7,p1_8,p1_9,p1_10,p1_11,p1_12,p1_13,p1_14,p1_15,p1_16,p1_17,p1_18,p1_19,p1_20,p1_21,p1_22,p1_23,p1_24,p1_25,p1_26,p1_27,p1_28,p1_29,p1_30,p1_31,p1_32,p1_33,p1_34,p1_35,p1_36,p1_37,p1_38,p1_39,p1_40,p1_41,p1_42,p1_43,p1_44,p1_45,p1_46,p1_47,p1_48,p1_49,p1_50]

p2_data = [p2_1,p2_2,p2_3,p2_4,p2_5,p2_6,p2_7,p2_8,p2_9,p2_10,p2_11,p2_12,p2_13,p2_14,p2_15,p2_16,p2_17,p2_18,p2_19,p2_20,p2_21,p2_22,p2_23,p2_24,p2_25,p2_26,p2_27,p2_28,p2_29,p2_30,p2_31,p2_32,p2_33,p2_34,p2_35,p2_36,p2_37,p2_38,p2_39,p2_40,p2_41,p2_42,p2_43,p2_44,p2_45,p2_46,p2_47,p2_48,p2_49,p2_50]








################# Creation of the env : 



WIDTH = 16000.0
HEIGHT = 9000.0

CP  = 0
POD = 1

MAX_THRUST = 200

E = 0.00001

r = -1
turn = 0
sols_ct = 0
is_p2 = False
cp_ct = -1
laps = -1

pods = []
cps = []


class Collision:

    def __init__(self):
        self.a = -1
        self.b = -1
        self.t = -1

    def update(self,a,b,t):
        self.a = a
        self.b = b
        self.t = t


class Point:

    def __init__(self,x,y):
        self.x = x
        self.y = y

    def dist(self,p):
        return math.pow(math.pow(self.x-p.x,2)+math.pow(self.y-p.y,2),0.5)
    def dist2(self,p):
        return math.pow(self.x-p.x,2)+math.pow(self.y-p.y,2)
    def closest(self,a,b):
        da = b.y - a.y
        db = a.x - b.x
        c1 = da*a.x + db*a.y
        c2 = -db*self.x + da*self.y
        det = da*da + db*db
        cx = 0
        cy = 0

        if (det != 0):
            cx = (da*c1 - db*c2)/det
            cy = (da*c2 + db*c1)/det
        else:
            cx = self.x
            cy = self.y

        p = Point(cx,cy)
        return p

cache = []

class Unit(Point):

    def __init__(self,x,y,id,type,r,vx,vy):
        Point.__init__(self,x,y)
        self.id = id
        self.type = type
        self.r = r
        self.vx = vx
        self.vy = vy
        self.cache = [0,0,0,0]


    #virtual void bounce(Unit* u) {};

    def collision_time(self,u):
        if (self.vx == u.vx and self.vy == u.vy):
            return -1

        if u.type == CP:
            sr2 = 357604
        else:
            sr2 = 640000

        dx = self.x - u.x
        dy = self.y - u.y
        dvx = self.vx - u.vx
        dvy = self.vy - u.vy
        a = dvx**2 + dvy**2

        if (a < E):
            return -1

        b = -2.0*(dx*dvx + dy*dvy)
        delta = b**2 - 4.0*a*(dx**2 + dy**2 - sr2)

        if (delta < 0.0):
            return -1


        t = (b - math.pow(delta,0.5))*(1.0/(2.0*a))

        if (t <= 0.0 or t > 1):
            return -1

        return t

    def save(self):
        self.cache[0] = self.x
        self.cache[1] = self.y
        self.cache[2] = self.vx
        self.cache[3] = self.vy


    def load(self):
        self.x = self.cache[0]
        self.y = self.cache[1]
        self.vx = self.cache[2]
        self.vy = self.cache[3]




class Checkpoint(Unit):

    def __init__(self,id,x,y):
        Unit.__init__(self,x,y,id,CP,600,0,0)

class Pod(Unit):

    def __init__(self,id,x,y):
        Unit.__init__(self,x,y,id,POD,400,0,0)
        self.angle = -1
        self.next_angle = -1
        self.has_boost = True
        self.ncpid = 1
        self.checked = 0
        self.timeout = 100
        self.shield = 0
        self.partner = 0
        self.pod_cache = [0,0,0,0,0,0]
        self.is_p2 = False
        self.cp_ct = 1
        self.laps = 3


    def boost(self,thrust):
        if (self.shield > 0):
            return
        else:
            ra = self.angle * math.pi/180
            self.vx += math.cos(ra)*thrust
            self.vy = math.sin(ra)*thrust

    def apply(self,thrust,angle):
        angle = max(-18,min(18,angle))
        self.angle += angle

        if (self.angle >= 360):
            self.angle = self.angle - 360
        elif (self.angle <= 0):
            self.angle += 360

        if (thrust == -1):
            self.shield = 4
        else:
            self.boost(thrust)


    def get_angle(self,p):
        d = (self.dist(p)+0.01)
        dx = (p.x - self.x)/d
        dy = (p.y - self.y)/d

        a = math.acos(dx)*180/math.pi
        if (dy < 0):
            a = 360 - a
        return a


    def diff_angle(self,p):
        a = self.get_angle(p)
        if (self.angle <= a):
            right = a - self.angle
        else:
            right = 360 + a - self.angle

        if (self.angle >= a):
            left = self.angle - a
        else:
            left = self.angle + 360 - a

        if (right < left):
            return right
        else:
            return -left


    def rotate(self,p):
        a = self.diff_angle(p)
        a = max(-18,min(18,a))

        self.angle += a

        if (self.angle >= 360):
            self.angle = self.angle - 360
        elif (self.angle < 0):
            self.angle += 360


    def move(self,t):
        self.x += t*self.vx
        self.y += t*self.vy

    def end(self):
        self.x = math.trunc(self.x)
        self.y = math.trunc(self.y)
        self.vx = math.trunc(self.vx*0.85)
        self.vy = math.trunc(self.vy*0.85)
        if (self.checked >= self.cp_ct * self.laps):
            self.ncpid = 0
            self.checked = self.cp_ct * self.laps
        self.timeout -= 1
        if (self.shield > 0):
            self.shield -= 1

    def apply_bot(self,game):
        cp = game.cps[self.ncpid]

        t = Point(cp.x-3*self.vx,cp.y-3*self.vy)


        raw_angle = self.diff_angle(t)

        thrust = 200
        angle = max(-18,min(18,raw_angle))
        self.apply(thrust,angle)

    def bounce_w_pod(self,u):
        if (self.shield == 4):
            m1 = 10
        else:
            m1 = 1

        if (u.shield == 4):
            m2 = 10
        else:
            m2 = 1

        mcoeff = (m1 + m2)/(m1*m2)
        nx = self.x - u.x
        ny = self.y - u.y
        dst2 = nx**2 + ny**2
        dvx = self.vx - u.vx
        dvy = self.vy - u.vy
        prod = (nx*dvx + ny*dvy)/(dst2 * mcoeff)
        fx = nx*prod
        fy = ny*prod
        m1_inv = 1.0/m1
        m2_inv = 1.0/m2

        self.vx -= fx*m1_inv
        self.vy -= fy*m1_inv
        u.vx += fx*m2_inv
        u.vy += fy*m2_inv

        impulse = math.pow(fx**2+fy**2,0.5)
        if (impulse < 120 and impulse > 0):
            df = 120.0/impulse
            fx *= df
            fy *= df

        self.vx -= fx*m1_inv
        self.vy -= fy*m1_inv
        u.vx += fx*m2_inv
        u.vy += fy*m2_inv


    def bounce(self,u):
        if (u.type == CP):
            self.checked += 1
            self.timeout = 100
            #self.partner.timeout = 100
            self.ncpid = (self.ncpid + 1)%self.cp_ct
            return
        else:
            self.bounce_w_pod(u)



    def update(self,x,y,vx,vy,angle,ncpid):
        if (self.shield > 0):
            self.shield -= 1
        if (ncpid != self.ncpid):
            self.timeout = 100
            #self.partner.timeout = 100
            self.checked += 1
        else:
            self.timeout -= 1

        self.x = x
        self.y = y
        self.vx = vx
        self.vy = vy
        self.ncpid = ncpid

        if (self.is_p2 == True and self.id > 1):
            a = copy.deepcopy(self.next_angle)
            self.next_angle = angle
            self.angle = a
        self.angle = angle

        Unit.save(self)


    def update_bis(self,shield,has_boost):
        self.shield = shield
        self.has_boost = has_boost

    def save(self):
        Unit.save(self)
        self.pod_cache[0] = self.ncpid
        self.pod_cache[1] = self.checked
        self.pod_cache[2] = self.timeout
        self.pod_cache[3] = self.shield
        self.pod_cache[4] = self.angle
        self.pod_cache[5] = self.has_boost


    def load(self):
        Unit.load(self)
        self.ncpid   = self.pod_cache[0]
        self.checked = self.pod_cache[1]
        self.timeout = self.pod_cache[2]
        self.shield  = self.pod_cache[3]
        self.angle   = self.pod_cache[4]
        self.has_boost = self.pod_cache[5]



class Game:

    def __init__(self):
        self.laps = 0
        self.cp_ct = 0
        self.pods = []
        self.cps = []
        self.is_p2 = False
        self.seed_size = 50
        self.possible_actions = self._build_possible_actions()
        self.inputs = np.random.random_sample((1,13))



    def compute_reward(self):

        length_to_win = 3*len(self.cps)

        max_dist = 30000


        p0 = self.pods[0]
        p1 = self.pods[1]
        p2 = self.pods[2]
        p3 = self.pods[3]

        score = 0

        score0 = p0.checked/length_to_win - 0.0001*p0.dist(self.cps[p0.ncpid])/max_dist
        score1 = p1.checked/length_to_win - 0.0001*p1.dist(self.cps[p1.ncpid])/max_dist
        score2 = p2.checked/length_to_win - 0.0001*p2.dist(self.cps[p2.ncpid])/max_dist
        score3 = p3.checked/length_to_win - 0.0001*p3.dist(self.cps[p3.ncpid])/max_dist

        if score0 < score1:
            myBlocker = p0
        else:
            myBlocker = p1

        if score2 < score3:
            hisRunner = p3
        else:
            hisRunner = p2

        score += max(score0,score1) - max(score2,score3)

        score -= 0.0001*myBlocker.dist(hisRunner)/max_dist
        score -= 0.0001*myBlocker.dist(self.cps[hisRunner.ncpid])/max_dist

        return score

    ########## For HER : 

    def _generate_goal(self):

        proba_pod_1 = random.random()
        if proba_pod_1 < 0.5:
            target_cp = self.cps[self.pods[0].ncpid]
            indx_cp = self.pods[0].ncpid
            pod = self.pods[0]
        else:
            target_cp = self.cps[self.pods[1].ncpid]
            indx_cp = self.pods[1].ncpid
            pod = self.pods[1]

        new_game = copy.deepcopy(self)

        delta_x = np.random.randint(-100,100)
        delta_y = np.random.randint(-100,100)

        new_game.cps[indx_cp].x = pod.x + delta_x
        new_game.cps[indx_cp].y = pod.y + delta_y

        return new_game


############################

    def afficher(self):
        print(" Pod 1 : x ",self.pods[0].x," y ",self.pods[0].y," vx ",self.pods[0].vx," vy ",self.pods[0].vy)


    #def _build_possible_actions(self):
        #possible_actions = np.array([[-18, -1, -18, -1], [-18, -1, -18, 0], [-18, -1, -18, 50], [-18, -1, -18, 150], [-18, -1, -18, 200], [-18, -1, -10, -1], [-18, -1, -10, 0], [-18, -1, -10, 50], [-18, -1, -10, 150], [-18, -1, -10, 200], [-18, -1, -5, -1], [-18, -1, -5, 0], [-18, -1, -5, 50], [-18, -1, -5, 150], [-18, -1, -5, 200], [-18, -1, 0, -1], [-18, -1, 0, 0], [-18, -1, 0, 50], [-18, -1, 0, 150], [-18, -1, 0, 200], [-18, -1, 5, -1], [-18, -1, 5, 0], [-18, -1, 5, 50], [-18, -1, 5, 150], [-18, -1, 5, 200], [-18, -1, 10, -1], [-18, -1, 10, 0], [-18, -1, 10, 50], [-18, -1, 10, 150], [-18, -1, 10, 200], [-18, -1, 18, -1], [-18, -1, 18, 0], [-18, -1, 18, 50], [-18, -1, 18, 150], [-18, -1, 18, 200], [-18, 0, -18, -1], [-18, 0, -18, 0], [-18, 0, -18, 50], [-18, 0, -18, 150], [-18, 0, -18, 200], [-18, 0, -10, -1], [-18, 0, -10, 0], [-18, 0, -10, 50], [-18, 0, -10, 150], [-18, 0, -10, 200], [-18, 0, -5, -1], [-18, 0, -5, 0], [-18, 0, -5, 50], [-18, 0, -5, 150], [-18, 0, -5, 200], [-18, 0, 0, -1], [-18, 0, 0, 0], [-18, 0, 0, 50], [-18, 0, 0, 150], [-18, 0, 0, 200], [-18, 0, 5, -1], [-18, 0, 5, 0], [-18, 0, 5, 50], [-18, 0, 5, 150], [-18, 0, 5, 200], [-18, 0, 10, -1], [-18, 0, 10, 0], [-18, 0, 10,50], [-18, 0, 10, 150], [-18, 0, 10, 200], [-18, 0, 18, -1], [-18, 0, 18, 0], [-18, 0, 18, 50], [-18, 0, 18, 150], [-18, 0, 18, 200], [-18, 50, -18, -1], [-18, 50, -18, 0], [-18, 50, -18, 50], [-18, 50, -18, 150], [-18, 50, -18, 200], [-18, 50, -10, -1], [-18, 50, -10, 0], [-18, 50, -10, 50], [-18, 50, -10, 150], [-18, 50, -10, 200],[-18, 50, -5, -1], [-18, 50, -5, 0], [-18, 50, -5, 50], [-18, 50, -5, 150], [-18, 50, -5, 200], [-18, 50, 0, -1], [-18, 50, 0, 0], [-18, 50, 0, 50], [-18, 50, 0, 150],[-18, 50, 0, 200], [-18, 50, 5, -1], [-18, 50, 5, 0], [-18, 50, 5, 50], [-18, 50, 5, 150], [-18, 50, 5, 200], [-18, 50, 10, -1], [-18, 50, 10, 0], [-18, 50, 10, 50], [-18, 50, 10, 150], [-18, 50, 10, 200], [-18, 50, 18, -1], [-18, 50, 18, 0], [-18, 50, 18, 50], [-18, 50, 18, 150], [-18, 50, 18, 200], [-18, 150, -18, -1], [-18, 150, -18, 0], [-18, 150, -18, 50], [-18, 150, -18, 150], [-18, 150, -18, 200], [-18, 150, -10, -1], [-18, 150, -10, 0], [-18, 150, -10, 50], [-18, 150, -10, 150], [-18, 150, -10, 200], [-18, 150, -5, -1], [-18, 150, -5, 0], [-18, 150, -5, 50], [-18, 150, -5, 150], [-18, 150, -5, 200], [-18, 150, 0, -1], [-18, 150, 0, 0], [-18, 150, 0, 50], [-18, 150, 0, 150], [-18, 150, 0, 200], [-18, 150, 5, -1], [-18, 150, 5, 0], [-18, 150, 5, 50], [-18, 150, 5, 150], [-18, 150, 5, 200], [-18, 150, 10, -1], [-18, 150, 10, 0], [-18, 150, 10, 50], [-18, 150, 10, 150], [-18, 150, 10, 200], [-18, 150, 18, -1], [-18, 150, 18, 0], [-18, 150, 18, 50], [-18, 150, 18, 150], [-18, 150, 18, 200], [-18, 200, -18, -1], [-18, 200, -18, 0], [-18, 200, -18, 50], [-18, 200, -18, 150], [-18, 200, -18, 200], [-18, 200, -10, -1], [-18, 200, -10, 0], [-18, 200, -10, 50], [-18, 200, -10, 150], [-18, 200, -10, 200], [-18, 200, -5, -1], [-18, 200, -5, 0], [-18, 200, -5, 50], [-18, 200, -5, 150], [-18, 200, -5, 200], [-18, 200, 0, -1], [-18, 200, 0, 0], [-18, 200, 0, 50], [-18, 200, 0, 150], [-18, 200, 0, 200], [-18, 200, 5, -1], [-18, 200, 5, 0], [-18, 200, 5, 50], [-18, 200, 5, 150], [-18, 200, 5, 200], [-18, 200, 10, -1], [-18, 200, 10, 0], [-18, 200, 10, 50], [-18, 200, 10, 150], [-18, 200, 10, 200], [-18, 200, 18, -1], [-18, 200, 18, 0], [-18, 200, 18, 50], [-18,200, 18, 150], [-18, 200, 18, 200], [-10, -1, -18, -1], [-10, -1, -18, 0], [-10, -1, -18, 50], [-10, -1, -18, 150], [-10, -1, -18, 200], [-10, -1, -10, -1], [-10, -1, -10, 0], [-10, -1, -10, 50], [-10, -1, -10, 150], [-10, -1, -10, 200], [-10, -1, -5, -1], [-10, -1, -5, 0], [-10, -1, -5, 50], [-10, -1, -5, 150], [-10, -1, -5, 200], [-10, -1, 0, -1], [-10, -1, 0, 0], [-10, -1, 0, 50], [-10, -1, 0, 150], [-10, -1, 0, 200], [-10, -1, 5, -1], [-10, -1, 5, 0], [-10, -1, 5, 50], [-10, -1, 5, 150], [-10, -1, 5, 200], [-10, -1, 10, -1], [-10, -1, 10, 0], [-10, -1, 10, 50], [-10, -1, 10, 150], [-10, -1, 10, 200], [-10, -1, 18, -1], [-10, -1, 18, 0], [-10, -1, 18, 50], [-10, -1, 18, 150], [-10, -1, 18, 200], [-10, 0, -18, -1], [-10, 0, -18, 0], [-10, 0, -18, 50], [-10, 0, -18, 150], [-10, 0, -18, 200], [-10, 0, -10, -1], [-10, 0, -10, 0], [-10, 0, -10, 50], [-10, 0, -10, 150], [-10, 0, -10, 200], [-10, 0, -5, -1], [-10, 0, -5, 0], [-10, 0, -5, 50], [-10, 0, -5, 150], [-10, 0, -5, 200], [-10, 0, 0, -1],[-10, 0, 0, 0], [-10, 0, 0, 50], [-10, 0, 0, 150], [-10, 0, 0, 200], [-10, 0, 5, -1], [-10, 0, 5, 0], [-10, 0, 5, 50], [-10, 0, 5, 150], [-10, 0, 5, 200], [-10, 0, 10,-1], [-10, 0, 10, 0], [-10, 0, 10, 50], [-10, 0, 10, 150], [-10, 0, 10, 200], [-10, 0, 18, -1], [-10, 0, 18, 0], [-10, 0, 18, 50], [-10, 0, 18, 150], [-10, 0, 18, 200], [-10, 50, -18, -1], [-10, 50, -18, 0], [-10, 50, -18, 50], [-10, 50, -18, 150], [-10, 50, -18, 200], [-10, 50, -10, -1], [-10, 50, -10, 0], [-10, 50, -10, 50], [-10, 50, -10, 150], [-10, 50, -10, 200], [-10, 50, -5, -1], [-10, 50, -5, 0], [-10, 50, -5, 50], [-10, 50, -5, 150], [-10, 50, -5, 200], [-10, 50, 0, -1], [-10, 50, 0, 0], [-10, 50, 0, 50], [-10, 50, 0, 150], [-10, 50, 0, 200], [-10, 50, 5, -1], [-10, 50, 5, 0], [-10, 50, 5, 50], [-10, 50, 5, 150], [-10, 50, 5, 200], [-10, 50, 10, -1], [-10, 50, 10, 0], [-10, 50, 10, 50], [-10, 50, 10, 150], [-10, 50, 10, 200], [-10, 50, 18, -1], [-10, 50, 18, 0], [-10, 50, 18, 50], [-10, 50, 18, 150], [-10, 50, 18, 200], [-10, 150, -18, -1], [-10, 150, -18, 0], [-10, 150, -18, 50], [-10, 150, -18, 150], [-10, 150, -18, 200], [-10, 150, -10, -1], [-10, 150, -10, 0], [-10, 150, -10, 50], [-10, 150, -10, 150], [-10, 150, -10, 200], [-10, 150, -5, -1], [-10, 150, -5, 0], [-10, 150, -5, 50], [-10, 150, -5, 150], [-10, 150, -5, 200], [-10, 150, 0, -1], [-10, 150, 0, 0], [-10, 150, 0, 50], [-10, 150, 0, 150], [-10, 150, 0, 200], [-10, 150, 5, -1], [-10, 150, 5, 0], [-10, 150, 5, 50], [-10, 150, 5, 150], [-10, 150, 5, 200], [-10, 150, 10, -1], [-10, 150, 10, 0], [-10, 150, 10, 50], [-10, 150, 10, 150], [-10, 150, 10, 200], [-10, 150, 18, -1], [-10, 150, 18, 0], [-10, 150, 18, 50], [-10,150, 18, 150], [-10, 150, 18, 200], [-10, 200, -18, -1], [-10, 200, -18, 0], [-10, 200, -18, 50], [-10, 200, -18, 150], [-10, 200, -18, 200], [-10, 200, -10, -1], [-10, 200, -10, 0], [-10, 200, -10, 50], [-10, 200, -10, 150], [-10, 200, -10, 200], [-10, 200, -5, -1], [-10, 200, -5, 0], [-10, 200, -5, 50], [-10, 200, -5, 150], [-10, 200, -5, 200], [-10, 200, 0, -1], [-10, 200, 0, 0], [-10, 200, 0, 50], [-10, 200, 0, 150], [-10, 200, 0, 200], [-10, 200, 5, -1], [-10, 200, 5, 0], [-10, 200, 5, 50], [-10, 200, 5, 150], [-10, 200, 5, 200], [-10, 200, 10, -1], [-10, 200, 10, 0], [-10, 200, 10, 50], [-10, 200, 10, 150], [-10, 200, 10, 200], [-10, 200, 18, -1], [-10, 200, 18, 0], [-10, 200, 18, 50], [-10, 200, 18, 150], [-10, 200, 18, 200], [-5, -1, -18, -1], [-5, -1, -18, 0], [-5, -1, -18, 50], [-5, -1, -18, 150], [-5, -1, -18, 200], [-5, -1, -10, -1], [-5, -1, -10, 0], [-5, -1, -10, 50], [-5, -1, -10, 150], [-5, -1, -10, 200], [-5, -1, -5, -1], [-5, -1, -5, 0], [-5, -1, -5, 50], [-5, -1, -5, 150], [-5, -1, -5, 200], [-5, -1, 0, -1], [-5, -1, 0, 0], [-5, -1, 0, 50], [-5, -1, 0, 150], [-5, -1, 0, 200], [-5, -1, 5, -1], [-5, -1, 5, 0], [-5, -1, 5, 50], [-5, -1, 5, 150], [-5, -1, 5, 200], [-5, -1, 10, -1], [-5, -1, 10, 0], [-5, -1, 10, 50], [-5, -1, 10, 150], [-5, -1, 10, 200], [-5, -1, 18, -1], [-5, -1, 18, 0], [-5, -1, 18, 50], [-5, -1, 18, 150], [-5, -1, 18, 200], [-5, 0, -18, -1], [-5, 0, -18, 0], [-5, 0, -18, 50], [-5, 0, -18, 150], [-5, 0, -18, 200], [-5, 0, -10, -1], [-5, 0, -10, 0], [-5, 0, -10, 50], [-5, 0, -10, 150], [-5, 0, -10, 200], [-5, 0, -5, -1], [-5, 0, -5, 0], [-5, 0, -5, 50], [-5, 0, -5, 150], [-5, 0, -5, 200], [-5, 0, 0, -1], [-5, 0, 0, 0], [-5, 0, 0, 50], [-5, 0, 0, 150], [-5, 0, 0, 200], [-5, 0, 5, -1], [-5, 0, 5, 0], [-5, 0, 5, 50], [-5, 0, 5, 150], [-5, 0, 5, 200], [-5, 0, 10, -1], [-5, 0, 10, 0], [-5,0, 10, 50], [-5, 0, 10, 150], [-5, 0, 10, 200], [-5, 0, 18, -1], [-5, 0, 18, 0], [-5, 0, 18, 50], [-5, 0, 18, 150], [-5, 0, 18, 200], [-5, 50, -18, -1], [-5, 50, -18, 0], [-5, 50, -18, 50], [-5, 50, -18, 150], [-5, 50, -18, 200], [-5, 50, -10, -1], [-5, 50, -10, 0], [-5, 50, -10, 50], [-5, 50, -10, 150], [-5, 50, -10, 200], [-5, 50, -5, -1], [-5, 50, -5, 0], [-5, 50, -5, 50], [-5, 50, -5, 150], [-5, 50, -5, 200], [-5, 50, 0, -1], [-5, 50, 0, 0], [-5, 50, 0, 50], [-5, 50, 0, 150], [-5, 50, 0, 200], [-5, 50, 5, -1], [-5, 50, 5, 0], [-5, 50, 5, 50], [-5, 50, 5, 150], [-5, 50, 5, 200], [-5, 50, 10, -1], [-5, 50, 10, 0], [-5, 50, 10, 50], [-5, 50, 10, 150], [-5, 50, 10, 200], [-5, 50, 18, -1], [-5, 50, 18, 0], [-5, 50, 18, 50], [-5, 50, 18, 150], [-5, 50, 18, 200], [-5, 150, -18, -1], [-5, 150, -18, 0], [-5, 150, -18, 50], [-5, 150,-18, 150], [-5, 150, -18, 200], [-5, 150, -10, -1], [-5, 150, -10, 0], [-5, 150, -10, 50], [-5, 150, -10, 150], [-5, 150, -10, 200], [-5, 150, -5, -1], [-5, 150, -5, 0], [-5, 150, -5, 50], [-5, 150, -5, 150], [-5, 150, -5, 200], [-5, 150, 0, -1], [-5, 150, 0, 0], [-5, 150, 0, 50], [-5, 150, 0, 150], [-5, 150, 0, 200], [-5, 150, 5, -1], [-5, 150, 5, 0], [-5, 150, 5, 50], [-5, 150, 5, 150], [-5, 150, 5, 200], [-5, 150, 10, -1], [-5, 150, 10, 0], [-5, 150, 10, 50], [-5, 150, 10, 150], [-5, 150, 10, 200], [-5, 150, 18, -1], [-5, 150, 18, 0], [-5, 150, 18, 50], [-5, 150, 18, 150], [-5, 150, 18, 200], [-5, 200, -18, -1], [-5, 200, -18, 0], [-5, 200, -18, 50], [-5, 200,-18, 150], [-5, 200, -18, 200], [-5, 200, -10, -1], [-5, 200, -10, 0], [-5, 200, -10, 50], [-5, 200, -10, 150], [-5, 200, -10, 200], [-5, 200, -5, -1], [-5, 200, -5, 0], [-5, 200, -5, 50], [-5, 200, -5, 150], [-5, 200, -5, 200], [-5, 200, 0, -1], [-5, 200, 0, 0], [-5, 200, 0, 50], [-5, 200, 0, 150], [-5, 200, 0, 200], [-5, 200, 5, -1], [-5, 200, 5, 0], [-5, 200, 5, 50], [-5, 200, 5, 150], [-5, 200, 5, 200], [-5, 200, 10, -1], [-5, 200, 10, 0], [-5, 200, 10, 50], [-5, 200, 10, 150], [-5, 200, 10, 200], [-5, 200, 18, -1], [-5, 200, 18, 0], [-5, 200, 18, 50], [-5, 200, 18, 150], [-5, 200, 18, 200], [0, -1, -18, -1], [0, -1, -18, 0], [0, -1, -18, 50], [0, -1, -18, 150], [0, -1, -18, 200], [0, -1, -10, -1], [0, -1, -10, 0], [0, -1, -10, 50], [0, -1, -10, 150], [0, -1, -10, 200], [0, -1, -5, -1], [0, -1, -5, 0], [0, -1, -5, 50], [0, -1, -5, 150], [0, -1, -5, 200], [0, -1, 0, -1], [0, -1, 0, 0], [0, -1, 0, 50], [0, -1, 0, 150], [0, -1, 0, 200], [0, -1, 5, -1], [0, -1, 5, 0], [0, -1, 5, 50], [0, -1, 5, 150], [0, -1, 5, 200], [0, -1, 10, -1], [0, -1, 10, 0], [0, -1, 10, 50], [0, -1, 10, 150], [0, -1, 10, 200], [0, -1, 18, -1], [0, -1, 18, 0], [0, -1, 18, 50], [0, -1, 18, 150], [0, -1, 18, 200], [0, 0, -18, -1], [0, 0, -18, 0], [0, 0, -18, 50], [0, 0, -18, 150], [0, 0, -18, 200], [0, 0, -10, -1], [0, 0, -10, 0], [0, 0, -10, 50], [0, 0, -10, 150], [0, 0, -10, 200], [0, 0, -5, -1], [0, 0, -5, 0], [0, 0, -5, 50], [0, 0, -5, 150], [0, 0, -5, 200], [0, 0, 0, -1], [0, 0, 0, 0], [0, 0, 0, 50], [0, 0, 0,150], [0, 0, 0, 200], [0, 0, 5, -1], [0, 0, 5, 0], [0, 0, 5, 50], [0, 0, 5, 150], [0, 0, 5, 200], [0, 0, 10, -1], [0, 0, 10, 0], [0, 0, 10, 50], [0, 0, 10, 150], [0, 0, 10, 200], [0, 0, 18, -1], [0, 0, 18, 0], [0, 0, 18, 50], [0, 0, 18, 150], [0, 0, 18, 200], [0, 50, -18, -1], [0, 50, -18, 0], [0, 50, -18, 50], [0, 50, -18, 150], [0,50, -18, 200], [0, 50, -10, -1], [0, 50, -10, 0], [0, 50, -10, 50], [0, 50, -10, 150], [0, 50, -10, 200], [0, 50, -5, -1], [0, 50, -5, 0], [0, 50, -5, 50], [0, 50, -5,150], [0, 50, -5, 200], [0, 50, 0, -1], [0, 50, 0, 0], [0, 50, 0, 50], [0, 50, 0, 150], [0, 50, 0, 200], [0, 50, 5, -1], [0, 50, 5, 0], [0, 50, 5, 50], [0, 50, 5, 150], [0, 50, 5, 200], [0, 50, 10, -1], [0, 50, 10, 0], [0, 50, 10, 50], [0, 50, 10, 150], [0, 50, 10, 200], [0, 50, 18, -1], [0, 50, 18, 0], [0, 50, 18, 50], [0, 50, 18, 150], [0, 50, 18, 200], [0, 150, -18, -1], [0, 150, -18, 0], [0, 150, -18, 50], [0, 150, -18, 150], [0, 150, -18, 200], [0, 150, -10, -1], [0, 150, -10, 0], [0, 150, -10, 50], [0, 150, -10, 150], [0, 150, -10, 200], [0, 150, -5, -1], [0, 150, -5, 0], [0, 150, -5, 50], [0, 150, -5, 150], [0, 150, -5, 200], [0, 150, 0, -1], [0, 150, 0, 0], [0, 150, 0, 50], [0, 150, 0, 150], [0, 150, 0, 200], [0, 150, 5, -1], [0, 150, 5, 0], [0, 150, 5, 50], [0, 150, 5, 150], [0, 150, 5, 200], [0, 150, 10, -1], [0, 150,10, 0], [0, 150, 10, 50], [0, 150, 10, 150], [0, 150, 10, 200], [0, 150, 18, -1], [0, 150, 18, 0], [0, 150, 18, 50], [0, 150, 18, 150], [0, 150, 18, 200], [0, 200, -18, -1], [0, 200, -18, 0], [0, 200, -18, 50], [0, 200, -18, 150], [0, 200, -18, 200], [0, 200, -10, -1], [0, 200, -10, 0], [0, 200, -10, 50], [0, 200, -10, 150], [0, 200,-10, 200], [0, 200, -5, -1], [0, 200, -5, 0], [0, 200, -5, 50], [0, 200, -5, 150], [0, 200, -5, 200], [0, 200, 0, -1], [0, 200, 0, 0], [0, 200, 0, 50], [0, 200, 0, 150], [0, 200, 0, 200], [0, 200, 5, -1], [0, 200, 5, 0], [0, 200, 5, 50], [0, 200, 5, 150], [0, 200, 5, 200], [0, 200, 10, -1], [0, 200, 10, 0], [0, 200, 10, 50], [0, 200,10, 150], [0, 200, 10, 200], [0, 200, 18, -1], [0, 200, 18, 0], [0, 200, 18, 50], [0, 200, 18, 150], [0, 200, 18, 200], [5, -1, -18, -1], [5, -1, -18, 0], [5, -1, -18,50], [5, -1, -18, 150], [5, -1, -18, 200], [5, -1, -10, -1], [5, -1, -10, 0], [5, -1, -10, 50], [5, -1, -10, 150], [5, -1, -10, 200], [5, -1, -5, -1], [5, -1, -5, 0], [5, -1, -5, 50], [5, -1, -5, 150], [5, -1, -5, 200], [5, -1, 0, -1], [5, -1, 0, 0], [5, -1, 0, 50], [5, -1, 0, 150], [5, -1, 0, 200], [5, -1, 5, -1], [5, -1, 5, 0], [5,-1, 5, 50], [5, -1, 5, 150], [5, -1, 5, 200], [5, -1, 10, -1], [5, -1, 10, 0], [5, -1, 10, 50], [5, -1, 10, 150], [5, -1, 10, 200], [5, -1, 18, -1], [5, -1, 18, 0], [5, -1, 18, 50], [5, -1, 18, 150], [5, -1, 18, 200], [5, 0, -18, -1], [5, 0, -18, 0], [5, 0, -18, 50], [5, 0, -18, 150], [5, 0, -18, 200], [5, 0, -10, -1], [5, 0, -10, 0], [5, 0, -10, 50], [5, 0, -10, 150], [5, 0, -10, 200], [5, 0, -5, -1], [5, 0, -5, 0], [5, 0, -5, 50], [5, 0, -5, 150], [5, 0, -5, 200], [5, 0, 0, -1], [5, 0, 0, 0], [5,0, 0, 50], [5, 0, 0, 150], [5, 0, 0, 200], [5, 0, 5, -1], [5, 0, 5, 0], [5, 0, 5, 50], [5, 0, 5, 150], [5, 0, 5, 200], [5, 0, 10, -1], [5, 0, 10, 0], [5, 0, 10, 50], [5, 0, 10, 150], [5, 0, 10, 200], [5, 0, 18, -1], [5, 0, 18, 0], [5, 0, 18, 50], [5, 0, 18, 150], [5, 0, 18, 200], [5, 50, -18, -1], [5, 50, -18, 0], [5, 50, -18, 50], [5, 50, -18, 150], [5, 50, -18, 200], [5, 50, -10, -1], [5, 50, -10, 0], [5, 50, -10, 50], [5, 50, -10, 150], [5, 50, -10, 200], [5, 50, -5, -1], [5, 50, -5, 0], [5, 50,-5, 50], [5, 50, -5, 150], [5, 50, -5, 200], [5, 50, 0, -1], [5, 50, 0, 0], [5, 50, 0, 50], [5, 50, 0, 150], [5, 50, 0, 200], [5, 50, 5, -1], [5, 50, 5, 0], [5, 50, 5,50], [5, 50, 5, 150], [5, 50, 5, 200], [5, 50, 10, -1], [5, 50, 10, 0], [5, 50, 10, 50], [5, 50, 10, 150], [5, 50, 10, 200], [5, 50, 18, -1], [5, 50, 18, 0], [5, 50, 18, 50], [5, 50, 18, 150], [5, 50, 18, 200], [5, 150, -18, -1], [5, 150, -18, 0], [5, 150, -18, 50], [5, 150, -18, 150], [5, 150, -18, 200], [5, 150, -10, -1], [5, 150, -10, 0], [5, 150, -10, 50], [5, 150, -10, 150], [5, 150, -10, 200], [5, 150, -5, -1], [5, 150, -5, 0], [5, 150, -5, 50], [5, 150, -5, 150], [5, 150, -5, 200], [5, 150, 0, -1], [5, 150, 0, 0], [5, 150, 0, 50], [5, 150, 0, 150], [5, 150, 0, 200], [5, 150, 5, -1], [5, 150, 5, 0], [5, 150, 5, 50], [5, 150, 5, 150], [5, 150, 5, 200], [5, 150, 10, -1], [5, 150, 10, 0], [5, 150, 10, 50], [5, 150, 10, 150], [5, 150, 10, 200], [5, 150, 18, -1], [5, 150, 18, 0], [5, 150, 18, 50], [5, 150, 18, 150], [5, 150, 18, 200], [5, 200, -18, -1], [5, 200, -18, 0], [5, 200, -18, 50], [5, 200, -18, 150], [5, 200, -18, 200], [5, 200, -10, -1], [5, 200, -10, 0], [5, 200, -10, 50], [5, 200, -10, 150], [5, 200, -10, 200], [5, 200, -5, -1], [5, 200, -5, 0], [5, 200, -5, 50], [5, 200, -5, 150], [5, 200, -5, 200], [5, 200, 0, -1], [5, 200, 0, 0], [5, 200, 0,50], [5, 200, 0, 150], [5, 200, 0, 200], [5, 200, 5, -1], [5, 200, 5, 0], [5, 200, 5, 50], [5, 200, 5, 150], [5, 200, 5, 200], [5, 200, 10, -1], [5, 200, 10, 0], [5, 200, 10, 50], [5, 200, 10, 150], [5, 200, 10, 200], [5, 200, 18, -1], [5, 200, 18, 0], [5, 200, 18, 50], [5, 200, 18, 150], [5, 200, 18, 200], [10, -1, -18, -1], [10, -1, -18, 0], [10, -1, -18, 50], [10, -1, -18, 150], [10, -1, -18, 200], [10, -1, -10, -1], [10, -1, -10, 0], [10, -1, -10, 50], [10, -1, -10, 150], [10, -1, -10, 200], [10, -1, -5, -1], [10, -1, -5, 0], [10, -1, -5, 50], [10, -1, -5, 150], [10, -1, -5, 200], [10, -1, 0, -1], [10, -1, 0, 0], [10, -1, 0, 50], [10, -1, 0, 150], [10, -1, 0,200], [10, -1, 5, -1], [10, -1, 5, 0], [10, -1, 5, 50], [10, -1, 5, 150], [10, -1, 5, 200], [10, -1, 10, -1], [10, -1, 10, 0], [10, -1, 10, 50], [10, -1, 10, 150], [10, -1, 10, 200], [10, -1, 18, -1], [10, -1, 18, 0], [10, -1, 18, 50], [10, -1, 18, 150], [10, -1, 18, 200], [10, 0, -18, -1], [10, 0, -18, 0], [10, 0, -18, 50], [10, 0, -18, 150], [10, 0, -18, 200], [10, 0, -10, -1], [10, 0, -10, 0], [10, 0, -10, 50], [10, 0, -10, 150], [10, 0, -10, 200], [10, 0, -5, -1], [10, 0, -5, 0], [10, 0, -5, 50], [10, 0, -5, 150], [10, 0, -5, 200], [10, 0, 0, -1], [10, 0, 0, 0], [10, 0, 0, 50], [10, 0, 0, 150], [10, 0, 0, 200], [10, 0, 5, -1], [10, 0, 5, 0], [10, 0, 5, 50], [10, 0, 5, 150], [10, 0, 5, 200], [10, 0, 10, -1], [10, 0, 10, 0], [10, 0, 10, 50], [10, 0, 10, 150], [10, 0, 10, 200], [10, 0, 18, -1], [10, 0, 18, 0], [10, 0, 18, 50],[10, 0, 18, 150], [10, 0, 18, 200], [10, 50, -18, -1], [10, 50, -18, 0], [10, 50, -18, 50], [10, 50, -18, 150], [10, 50, -18, 200], [10, 50, -10, -1], [10, 50, -10, 0], [10, 50, -10, 50], [10, 50, -10, 150], [10, 50, -10, 200], [10, 50, -5, -1], [10, 50, -5, 0], [10, 50, -5, 50], [10, 50, -5, 150], [10, 50, -5, 200], [10, 50, 0, -1],[10, 50, 0, 0], [10, 50, 0, 50], [10, 50, 0, 150], [10, 50, 0, 200], [10, 50, 5, -1], [10, 50, 5, 0], [10, 50, 5, 50], [10, 50, 5, 150], [10, 50, 5, 200], [10, 50, 10,-1], [10, 50, 10, 0], [10, 50, 10, 50], [10, 50, 10, 150], [10, 50, 10, 200], [10, 50, 18, -1], [10, 50, 18, 0], [10, 50, 18, 50], [10, 50, 18, 150], [10, 50, 18, 200], [10, 150, -18, -1], [10, 150, -18, 0], [10, 150, -18, 50], [10, 150, -18, 150], [10, 150, -18, 200], [10, 150, -10, -1], [10, 150, -10, 0], [10, 150, -10, 50], [10, 150, -10, 150], [10, 150, -10, 200], [10, 150, -5, -1], [10, 150, -5, 0], [10, 150, -5, 50], [10, 150, -5, 150], [10, 150, -5, 200], [10, 150, 0, -1], [10, 150, 0, 0], [10, 150, 0, 50], [10, 150, 0, 150], [10, 150, 0, 200], [10, 150, 5, -1], [10, 150, 5, 0], [10, 150, 5, 50], [10, 150, 5, 150], [10, 150, 5, 200], [10, 150, 10, -1], [10, 150, 10, 0], [10, 150, 10, 50], [10, 150, 10, 150], [10, 150, 10, 200], [10, 150, 18, -1], [10, 150, 18, 0], [10, 150, 18, 50], [10, 150, 18, 150], [10, 150, 18, 200], [10, 200, -18, -1], [10, 200, -18, 0], [10, 200, -18, 50], [10, 200, -18, 150], [10, 200, -18, 200], [10, 200, -10, -1], [10, 200, -10, 0], [10, 200, -10, 50], [10, 200, -10, 150], [10, 200, -10, 200], [10, 200, -5, -1], [10, 200, -5, 0], [10, 200, -5, 50], [10, 200, -5, 150], [10, 200, -5, 200], [10, 200, 0, -1], [10, 200, 0, 0], [10, 200, 0, 50], [10, 200, 0, 150], [10, 200, 0, 200], [10, 200, 5, -1], [10, 200, 5, 0], [10, 200, 5, 50], [10, 200, 5, 150], [10, 200, 5, 200], [10, 200, 10, -1], [10, 200, 10, 0], [10, 200, 10, 50], [10, 200, 10, 150], [10, 200, 10, 200], [10, 200, 18, -1], [10, 200, 18, 0], [10, 200, 18, 50], [10, 200, 18, 150], [10, 200, 18, 200], [18, -1, -18, -1], [18, -1, -18, 0], [18, -1, -18, 50], [18, -1, -18, 150], [18, -1, -18, 200], [18, -1, -10, -1], [18, -1, -10, 0], [18, -1, -10, 50], [18, -1, -10, 150], [18, -1, -10, 200], [18, -1, -5, -1], [18, -1, -5, 0], [18, -1, -5, 50], [18, -1, -5, 150], [18, -1, -5, 200], [18, -1, 0, -1], [18, -1, 0, 0], [18, -1, 0, 50], [18, -1, 0, 150], [18, -1, 0, 200], [18, -1, 5, -1], [18, -1, 5, 0], [18, -1, 5, 50], [18, -1, 5, 150], [18, -1, 5, 200], [18, -1, 10, -1], [18, -1, 10, 0], [18, -1, 10,50], [18, -1, 10, 150], [18, -1, 10, 200], [18, -1, 18, -1], [18, -1, 18, 0], [18, -1, 18, 50], [18, -1, 18, 150], [18, -1, 18, 200], [18, 0, -18, -1], [18, 0, -18, 0], [18, 0, -18, 50], [18, 0, -18, 150], [18, 0, -18, 200], [18, 0, -10, -1], [18, 0, -10, 0], [18, 0, -10, 50], [18, 0, -10, 150], [18, 0, -10, 200], [18, 0, -5, -1], [18, 0, -5, 0], [18, 0, -5, 50], [18, 0, -5, 150], [18, 0, -5, 200], [18, 0, 0, -1], [18, 0, 0, 0], [18, 0, 0, 50], [18, 0, 0, 150], [18, 0, 0, 200], [18, 0, 5, -1], [18,0, 5, 0], [18, 0, 5, 50], [18, 0, 5, 150], [18, 0, 5, 200], [18, 0, 10, -1], [18, 0, 10, 0], [18, 0, 10, 50], [18, 0, 10, 150], [18, 0, 10, 200], [18, 0, 18, -1], [18,0, 18, 0], [18, 0, 18, 50], [18, 0, 18, 150], [18, 0, 18, 200], [18, 50, -18, -1], [18, 50, -18, 0], [18, 50, -18, 50], [18, 50, -18, 150], [18, 50, -18, 200], [18, 50, -10, -1], [18, 50, -10, 0], [18, 50, -10, 50], [18, 50, -10, 150], [18, 50, -10, 200], [18, 50, -5, -1], [18, 50, -5, 0], [18, 50, -5, 50], [18, 50, -5, 150], [18, 50, -5, 200], [18, 50, 0, -1], [18, 50, 0, 0], [18, 50, 0, 50], [18, 50, 0, 150], [18, 50, 0, 200], [18, 50, 5, -1], [18, 50, 5, 0], [18, 50, 5, 50], [18, 50, 5, 150], [18, 50, 5, 200], [18, 50, 10, -1], [18, 50, 10, 0], [18, 50, 10, 50], [18, 50, 10, 150], [18, 50, 10, 200], [18, 50, 18, -1], [18, 50, 18, 0], [18, 50, 18, 50], [18, 50,18, 150], [18, 50, 18, 200], [18, 150, -18, -1], [18, 150, -18, 0], [18, 150, -18, 50], [18, 150, -18, 150], [18, 150, -18, 200], [18, 150, -10, -1], [18, 150, -10, 0], [18, 150, -10, 50], [18, 150, -10, 150], [18, 150, -10, 200], [18, 150, -5, -1], [18, 150, -5, 0], [18, 150, -5, 50], [18, 150, -5, 150], [18, 150, -5, 200], [18, 150, 0, -1], [18, 150, 0, 0], [18, 150, 0, 50], [18, 150, 0, 150], [18, 150, 0, 200], [18, 150, 5, -1], [18, 150, 5, 0], [18, 150, 5, 50], [18, 150, 5, 150], [18, 150, 5, 200], [18, 150, 10, -1], [18, 150, 10, 0], [18, 150, 10, 50], [18, 150, 10, 150], [18, 150, 10, 200], [18, 150, 18, -1], [18, 150, 18, 0], [18, 150, 18, 50], [18, 150, 18, 150], [18, 150, 18, 200], [18, 200, -18, -1], [18, 200, -18, 0], [18, 200, -18, 50], [18, 200, -18, 150], [18, 200, -18, 200], [18, 200, -10, -1], [18, 200, -10, 0], [18, 200, -10, 50], [18, 200, -10, 150], [18, 200, -10, 200], [18, 200, -5, -1], [18, 200, -5, 0], [18, 200, -5, 50], [18, 200, -5, 150], [18, 200, -5, 200], [18, 200, 0, -1], [18, 200, 0, 0], [18, 200, 0, 50], [18, 200, 0, 150], [18, 200, 0, 200], [18, 200, 5, -1], [18, 200, 5, 0], [18, 200, 5, 50], [18, 200, 5, 150], [18, 200, 5, 200], [18, 200, 10, -1], [18, 200, 10, 0], [18, 200, 10, 50], [18, 200, 10, 150], [18, 200, 10, 200], [18, 200, 18, -1], [18, 200, 18, 0], [18, 200, 18, 50], [18, 200, 18, 150], [18, 200, 18, 200]])
        #return possible_actions

    def _build_possible_actions(self):
        possible_actions = np.array([[-18,0],[-18,50],[-18,100],[-18,150],[-18,200],[-15,0],[-15,50],[-15,100],[-15,150],[-15,200],[-10,0],[-10,50],[-10,100],[-10,150],[-10,200],[-5,0],[-5,50],[-5,100],[-5,150],[-5,200],[0,0],[0,50],[0,100],[0,150],[0,200],[18,0],[18,50],[18,100],[18,150],[18,200],[15,0],[15,50],[15,100],[15,150],[15,200],[10,0],[10,50],[10,100],[10,150],[10,200],[5,0],[5,50],[5,100],[5,150],[5,200]])
        return possible_actions




    def _build_inputs(self):
        inputs = np.random.random_sample((1,13))
        compteur = 0
        for pod in self.pods:
            inputs[0][compteur] = pod.x/16000
            compteur += 1

            inputs[0][compteur] = pod.y/9000
            compteur += 1

            inputs[0][compteur] = pod.vx/1000
            compteur += 1

            inputs[0][compteur] = pod.vy/1000
            compteur += 1

            inputs[0][compteur] = pod.timeout/500
            compteur += 1

            inputs[0][compteur] = pod.shield/4
            compteur += 1

            inputs[0][compteur] = pod.ncpid/len(self.cps)
            compteur += 1

            inputs[0][compteur] = pod.angle/360
            compteur += 1

            inputs[0][compteur] = pod.checked/len(self.cps * 3)
            compteur += 1

            cP1 = self.cps[pod.ncpid]
            cP2 = self.cps[(pod.ncpid+1)%len(self.cps)]

            inputs[0][compteur] = cP1.x/16000
            compteur += 1

            inputs[0][compteur] = cP1.y/9000
            compteur += 1

            inputs[0][compteur] = cP2.x/16000
            compteur += 1

            inputs[0][compteur] = cP2.y/9000
            compteur += 1

        return inputs


    def _refresh_inputs(self):

        compteur = 0
        for pod in self.pods:
            self.inputs[0][compteur] = pod.x/16000
            compteur += 1

            self.inputs[0][compteur] = pod.y/9000
            compteur += 1

            self.inputs[0][compteur] = pod.vx/1000
            compteur += 1

            self.inputs[0][compteur] = pod.vy/1000
            compteur += 1

            self.inputs[0][compteur] = pod.timeout/100
            compteur += 1

            self.inputs[0][compteur] = pod.shield/4
            compteur += 1

            self.inputs[0][compteur] = pod.ncpid/len(self.cps)
            compteur += 1

            self.inputs[0][compteur] = pod.angle/360
            compteur += 1

            self.inputs[0][compteur] = pod.checked/(len(self.cps)*3)
            compteur += 1

            cP1 = self.cps[pod.ncpid]
            cP2 = self.cps[(pod.ncpid+1)%len(self.cps)]

            self.inputs[0][compteur] = cP1.x/16000
            compteur += 1

            self.inputs[0][compteur] = cP1.y/9000
            compteur += 1

            self.inputs[0][compteur] = cP2.x/16000
            compteur += 1

            self.inputs[0][compteur] = cP2.y/9000
            compteur += 1


    def _opponent_inputs(self):
        inputs = np.random.random_sample((1,13))
        compteur = 0
        for i in range(4):
            if i == 0:
                pod = self.pods[2]
            elif i == 1:
                pod = self.pods[3]
            elif i == 2:
                pod = self.pods[0]
            else:
                pod = self.pods[1]
            inputs[0][compteur] = pod.x/16000
            compteur += 1

            inputs[0][compteur] = pod.y/9000
            compteur += 1

            inputs[0][compteur] = pod.vx/1000
            compteur += 1

            inputs[0][compteur] = pod.vy/1000
            compteur += 1

            inputs[0][compteur] = pod.timeout/100
            compteur += 1

            inputs[0][compteur] = pod.shield/4
            compteur += 1

            inputs[0][compteur] = pod.ncpid/len(self.cps)
            compteur += 1

            inputs[0][compteur] = pod.angle/360
            compteur += 1

            inputs[0][compteur] = pod.checked/(len(self.cps)*3)
            compteur += 1

            cP1 = self.cps[pod.ncpid]
            cP2 = self.cps[(pod.ncpid+1)%len(self.cps)]

            inputs[0][compteur] = cP1.x/16000
            compteur += 1

            inputs[0][compteur] = cP1.y/9000
            compteur += 1

            inputs[0][compteur] = cP2.x/16000
            compteur += 1

            inputs[0][compteur] = cP2.y/9000
            compteur += 1

        return inputs


    def get_state_size(self):
        return 13

    def get_action_size(self):
        return len(self.possible_actions)


    def generate_random_game(self):
        proba = random.random()
        seed = random.randint(0,self.seed_size-1)
        if proba < 0.7:
            seed = random.randint(0,self.seed_size-1)
            self.laps = 3 #random.randint()
            random_cp = cp_data[seed]
        else:
            taille = np.random.randint(2,4)
            random_cp = []
            self.laps = 3 #random.randint()
            for _ in range(taille):
                random_cp.append([np.random.randint(0,16000),np.random.randint(0,9000)])

        self.laps = 3 #random.randint()
        #random_cp = cp_data[seed]
        random_p1 = p1_data[seed]
        random_p2 = p2_data[seed]

        randomX = np.random.randint(0,16000)
        randomY = np.random.randint(0,9000)
        randomVx = np.random.randint(0,400)
        randomVy = np.random.randint(0,400)
        randomAngle = np.random.randint(0,360)

        for i in range(len(random_cp)):
            cp_ = Checkpoint(i,random_cp[i][0],random_cp[i][1])
            self.cps.append(cp_)
        self.cp_ct = len(random_cp)

        if (random.random() < 0.5):
            self.player = False
        else:
            self.player = True

        if (self.is_p2 == False):

            pod_1 = Pod(0,randomX,randomY)
            #pod_2 = Pod(1,random_p1[1][0],random_p1[1][1])
            #pod_3 = Pod(2,random_p2[0][0],random_p2[0][1])
            #pod_4 = Pod(3,random_p2[1][0],random_p2[1][1])

            pod_1.angle = randomAngle
            #pod_2.angle = random_p2[1][2]
            #pod_3.angle = random_p2[0][2]
            #pod_4.angle = random_p2[1][2]

            #pod_1.partner = pod_2
            #pod_2.partner = pod_1

            #pod_3.partner = pod_4
            #pod_4.partner = pod_3

            #pod_3.is_p2 = True
            #pod_4.is_p2 = True

        else:

            pod_1 = Pod(0,randomX,randomY)
            #pod_2 = Pod(1,random_p2[1][0],random_p2[1][1])
            #pod_3 = Pod(2,random_p1[0][0],random_p1[0][1])
            #pod_4 = Pod(3,random_p1[1][0],random_p1[1][1])

            pod_1.angle = randomAngle
            #pod_2.angle = random_p2[1][2]
            #pod_3.angle = random_p2[0][2]
            #pod_4.angle = random_p2[1][2]

            #pod_1.partner = pod_2
            #pod_2.partner = pod_1

            #pod_3.partner = pod_4
            #pod_4.partner = pod_3

            pod_1.is_p2 = True
            #pod_2.is_p2 = True

        pod_1.ncpid = 1
        pod_1.timeout = 100
        #pod_2.ncpid = 1
        #pod_2.timeout = 100
        #pod_3.ncpid = 1
        #pod_3.timeout = 100
        #pod_4.ncpid = 1
        #pod_4.timeout = 100
        pod_1.vx = randomVx
        pod_1.vy = randomVy
        pod_1.cp_ct = self.cp_ct
        #pod_2.cp_ct = self.cp_ct
        #pod_3.cp_ct = self.cp_ct
        #pod_4.cp_ct = self.cp_ct

        self.pods.append(pod_1)
        #self.pods.append(pod_2)
        #self.pods.append(pod_3)
        #self.pods.append(pod_4)


    def play(self):
        t = 0.0
        while (t < 1.0):
            first_col = Collision()
            for i in range(1):
                for j in range(i+1,1):
                    col_time = self.pods[i].collision_time(self.pods[j])
                    if (col_time > -1 and col_time + t < 1.0 and (first_col.t == -1 or col_time < first_col.t)):
                        first_col.a = self.pods[i]
                        first_col.b = self.pods[j]
                        first_col.t = col_time

                col_time = self.pods[i].collision_time(self.cps[self.pods[i].ncpid])
                if (col_time > -1 and col_time + t < 1.0 and (first_col.t == -1 or col_time < first_col.t)):
                        first_col.a = self.pods[i]
                        first_col.b = self.cps[self.pods[i].ncpid]
                        first_col.t = col_time

            if (first_col.t == -1):
                for i in range(1):
                    self.pods[i].move(1.0-t)
                t = 1.0
            else:
                for i in range(1):
                    self.pods[i].move(first_col.t)
                first_col.a.bounce(first_col.b)
                t += first_col.t


        for i in range(1):
            self.pods[i].end()


    def step(self,action,opp_action):

        old_state = copy.deepcopy(self)


        a1 = action[0]
        s1 = action[1]
        #a2 = action[2]
        #s2 = action[3]

        #opp_a1 = opp_action[0]
        #opp_s1 = opp_action[1]
        #opp_a2 = opp_action[2]
        #opp_s2 = opp_action[3]

        self.pods[0].apply(s1,a1)
        #self.pods[1].apply(s2,a2)
        #self.pods[2].apply(opp_s1,opp_a1)
        #self.pods[3].apply(opp_s2,opp_a2)


        self.play()

        # Calcul d'un reward modifié


        reward = 0
        done = False
        to_return = []
        state = ""
        reset = False
        check = False

        #reward = self.compute_reward()

        if self.pods[0].x > 24000 or self.pods[0].x < - 8000 or self.pods[0].y < - 4500 or self.pods[0].y > 13500:
            self.pods[0].x  = np.random.randint(0,16000)
            self.pods[0].y =  np.random.randint(0,9000)
            self.pods[0].vx =  np.random.randint(0,400)
            self.pods[0].vy =  np.random.randint(0,400)
            self.pods[0].angle =  np.random.randint(0,360)
            reset = True
            #reward -= 1.0

        #if self.pods[0].timeout == 0:
            #done = True
            #reward -= 1.0
            #state += " - we timeout - "

        if self.pods[0].checked > old_state.pods[0].checked:
            reward += 1
            state += " - we passed cp nb "+str(self.pods[0].ncpid-1)+" - "
            check = True




        if self.pods[0].checked == len(self.cps)*3:
            reward += 1
            done = True
            state += " - we win - "



        return [old_state,self,reward,done,state,check,reset]



    def step_bis(self,action,opp_action):

        old_state = copy.deepcopy(self)


        a1 = action[0]
        s1 = action[1]


        self.pods[0].apply(s1,a1)

        self.play()

        reward = 0
        done = False
        reset = False
        to_return = []
        state = ""
        check = False

        if self.pods[0].x > 24000 or self.pods[0].x < - 8000 or self.pods[0].y < - 4500 or self.pods[0].y > 13500:
            self.pods[0].x  = np.random.randint(0,16000)
            self.pods[0].y =  np.random.randint(0,9000)
            self.pods[0].vx =  np.random.randint(0,400)
            self.pods[0].vy =  np.random.randint(0,400)
            self.pods[0].angle =  np.random.randint(0,360)
            reset = True


            #reward -= 1.0

        #if self.pods[0].timeout == 0:
            #done = True
            #reward -= 1.0
            #state += " - we timeout - "

        if self.pods[0].checked > old_state.pods[0].checked:
            reward += 1.0
            state += " - we passed cp nb "+str(self.pods[0].ncpid-1)+" - "
            check = True




        if self.pods[0].checked == len(self.cps)*3:
            reward += 1.0
            done = True
            state += " - we win - "


        return [old_state,self,reward,done,state,check,reset]



    def step_bot(self):

        old_state = copy.deepcopy(self)



        self.pods[0].apply_bot(self)
        self.pods[1].apply_bot(self)
        self.pods[2].apply_bot(self)
        self.pods[3].apply_bot(self)

        self.play()

        reward = 0
        done = False
        to_return = []
        state = ""

        if self.pods[0].x < - 1000 or self.pods[0].x > 17000:
            reward -= 0
        if self.pods[0].y < - 1000 or self.pods[0].y > 10000:
            reward -= 0

        if self.pods[1].x < - 1000 or self.pods[1].x > 17000:
            reward -= 0
        if self.pods[1].y < - 1000 or self.pods[1].y > 10000:
            reward -= 0


        if self.pods[0].timeout == 0 or self.pods[1].timeout == 0:
            reward -= 1
            done = True
            state += " - we timeout - "

        if self.pods[2].timeout == 0 or self.pods[3].timeout == 0:
            reward += 1
            done = True
            state += " - they timeout - "

        if self.pods[0].checked > old_state.pods[0].checked:
            reward += 10.0
            state += " - we passed cp nb "+str(self.pods[0].ncpid-1)+" - "

        if self.pods[1].checked > old_state.pods[1].checked:
            reward += 10.0
            state += " - we passed cp nb "+str(self.pods[1].ncpid-1)+" - "

        if self.pods[2].checked > old_state.pods[2].checked:
            reward -= 10.0
            state += " - they passed cp nb "+str(self.pods[2].ncpid-1)+" - "

        if self.pods[3].checked > old_state.pods[3].checked:
            reward -= 10.0
            state += " - they passed cp nb "+str(self.pods[3].ncpid-1)+" - "


        if self.pods[0].checked == len(self.cps)*3:
            reward += 50
            done = True
            state += " - we win - "

        if self.pods[0].checked == len(self.cps)*3:
            reward += 50
            done = True
            state += " - we win - "

        if self.pods[0].checked == len(self.cps)*3:
            reward -= 50
            done = True
            state += " - they win - "

        if self.pods[0].checked == len(self.cps)*3:
            reward -= 50
            done = True
            state += " - they win - "

        return [old_state,self,reward,done,state]


##### Creating our DQNA Agent : 
    
class DQNAgent:

    def __init__(self, state_size, action_size):
        self.state_size = state_size
        self.action_size = action_size
        self.memory = deque(maxlen=100000)
        self.good_memory = deque(maxlen = 10000)
        self.short_memory = []
        self.gamma = 0.99   
        self.epsilon = 1.0 
        self.epsilon_min = 0.1
        self.epsilon_decay = 0.9999
        self.learning_rate = 0.01
        self.future_p = 0
        self.proba_opp = 0.647
        self.pro = 0.722
        self.cheat_proba = 1.0
        self.model = self._build_model()
        self.target_model = self._build_model()
        self.update_target_model()
        self.p_good = 0.5
        self.bot_treshhold = 0.2

    def _huber_loss(self, y_true, y_pred, clip_delta=1.0):
        error = y_true - y_pred
        cond  = K.abs(error) <= clip_delta

        squared_loss = 0.5 * K.square(error)
        quadratic_loss = 0.5 * K.square(clip_delta) + clip_delta * (K.abs(error) - clip_delta)

        return K.mean(tf.where(cond, squared_loss, quadratic_loss))

    ### Pretty basic fully connected nn : 
    
    def _build_model(self):
        
        model = keras.Sequential()
        #indx = 0
        model.add(keras.layers.Dense(28, input_dim=self.state_size, activation='relu'))
        #indx = 1
        model.add(keras.layers.Dense(28,activation='relu'))
        #indx = 2
        model.add(keras.layers.Dense(28,activation = 'relu'))
        #indx = 3
        model.add(keras.layers.Dense(59, activation='relu'))
        #indx = 4
        model.add(keras.layers.Dense(self.action_size, activation='linear'))

        #model.get_layer(index=0).set_weights(L1)
        #model.get_layer(index=1).set_weights(L2)
        #model.get_layer(index=2).set_weights(L3)
        #model.get_layer(index=3).set_weights(L4)
        #model.get_layer(index=4).set_weights(L5)



        model.compile(loss=self._huber_loss,
                      optimizer=keras.optimizers.Adam(lr=0.0007, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)
)
        model.summary()
        return model

    def _get_p_good(self):
        if len(self.good_memory) < 64:
            return 1.0
        else:
            return 0.5


    def remember_good(self,state,action,reward,next_state,done):
        self.good_memory.append((state, action, reward, next_state, done))
        
        
## Here, we separated memories with good or bad reward, to maximize the training on the one with good reward.
## We also compute n-cumulativ reward, and we create new goal for HER : 

    def remember(self, state, action, reward, next_state, done,times,reset):
        if not done and times < 990:
            self.short_memory.append([state,action,reward,next_state,done])
            #current_epsilon = copy.deepcopy(self.epsilon)
            #self.epsilon = 1.0

            #state = copy.deepcopy(state)

            for i in range(self.future_p):
                action = agent.act(state)
                opp_action = agent.opp_act_train(state,self.proba_opp)

                if (self.pro < self.bot_treshhold):
                    tab = state.step_bis(state.possible_actions[action],state.possible_actions[opp_action])
                else:
                    tab = state.step(state.possible_actions[action],state.possible_actions[opp_action])

                next_env = tab[1]._generate_goal()
                env = tab[0]
                reward = 1.0
                done = tab[3]
                env._refresh_inputs()
                next_env._refresh_inputs()
                self.short_memory.append([env,action,reward,next_env,done])

            #print("checking ---&>")
            #next_env.afficher()
            #print("cps : ",next_env.cps[next_env.pods[0].ncpid].x," ",next_env.cps[next_env.pods[0].ncpid].y," ",next_env.cps[next_env.pods[1].ncpid].x," ",next_env.cps[next_env.pods[1].ncpid].y," ")

            #state = env

            #self.epsilon = current_epsilon
        elif reset:
            self.short_memory.append([state,action,reward,next_state,done])
            best_action = np.argmax(self.model.predict(next_state.inputs)[0])
            target = self.target_model.predict(next_state.inputs)[0][best_action]
            for i in range(len(self.short_memory)-1):
                R = 0
                for j in range(i,len(self.short_memory)-1):
                    R += self.short_memory[j][2]*(self.gamma**(j-i))
                self.short_memory[i][2] = R + target*self.gamma**(len(self.short_memory)-i)
                #print(self.short_memory[i][2])
                self.memory.append((self.short_memory[i][0], self.short_memory[i][1], self.short_memory[i][2],self.short_memory[i][3], self.short_memory[i][4]))
            self.short_memory = []
            self.memory.append((state, action, reward, next_state, done))
        else:
            self.short_memory.append([state,action,reward,next_state,done])
            best_action = np.argmax(self.model.predict(next_state.inputs)[0])
            target = self.target_model.predict(next_state.inputs)[0][best_action]
            for i in range(len(self.short_memory)-1):
                R = 0
                for j in range(i,len(self.short_memory)-1):
                    R += self.short_memory[j][2]*(self.gamma**(j-i))
                self.short_memory[i][2] = R + target*self.gamma**(len(self.short_memory)-i)
                #print(self.short_memory[i][2])
                self.memory.append((self.short_memory[i][0], self.short_memory[i][1], self.short_memory[i][2],self.short_memory[i][3], self.short_memory[i][4]))
            self.short_memory = []
            self.memory.append((state, action, reward, next_state, done))



    def act(self, state):
        proba = np.random.rand()

        if proba <= self.epsilon:
            indx =  random.randrange(self.action_size)
            return indx

        act_values = self.model.predict(state.inputs)
        indx = np.argmax(act_values[0])  # returns action
        return indx






    def opp_act(self, state):

        act_values = self.target_model.predict(state._opponent_inputs())
        indx = np.argmax(act_values[0])  # returns action
        return indx

    def opp_act_train(self, state,proba):
        indx =  random.randrange(self.action_size)
        return indx
    
  ## Rather than training at every step, we train fully at the end of every episode (for performance issues)

    def replay(self, batch_size):

        minibatch = random.sample(self.memory, batch_size)
        if len(self.good_memory) > 128/4:
            good_minibatch = random.sample(self.good_memory,batch_size/4)
        else:
            good_minibatch = self.good_memory

        for tab in good_minibatch:
            minibatch.append(tab)

        states, targets_f = [], []
        tab_simu_with_model,tab_simu_with_target_mode,tab_target_f = [],[],[]

        tab_state,tab_next_state,tab_reward = [],[],[]

        for state, action, reward, next_state, done in minibatch:
            tab_state.append(state.inputs)
            tab_next_state.append(next_state.inputs)
        tab_state = np.array(tab_state)
        tab_next_state = np.array(tab_next_state)

        model_predict_next_state = self.model.predict(tab_next_state)
        target_model_predict_next_state = self.target_model.predict(tab_next_state)
        model_predict_state = self.model.predict(tab_next_state)

        for i in range(len(minibatch)):

            target = reward
            if not done:
                simu_with_model = model_predict_next_state[i]
                best_action = np.argmax(simu_with_model)
                simu_with_target_mode = target_model_predict_next_state[i]
                target = reward + self.gamma * simu_with_target_mode[best_action]
            target_f = model_predict_state[i]
            target_f[0][action] = target

            # Filtering out states and targets for training
            states.append(state.inputs[0])
            targets_f.append(target_f[0])

        history = self.model.fit(np.array(states), np.array(targets_f),batch_size=None, epochs=1, verbose=0)
        # Keeping track of loss
        loss = history.history['loss'][0]
        if self.epsilon > self.epsilon_min:
            self.epsilon -= 0.000003
            self.epsilon -= 0.000003
        return loss

    def replay_full(self, batch_size,times):
        #print(" ------ Replay starting ------- ")
        full_minibatch = random.sample(self.memory, batch_size)
        print(len(self.good_memory))
        for _ in range(1,times):
            minibatch = random.sample(self.memory, batch_size)
            full_minibatch += minibatch

        states, targets_f = [], []
        tab_state,tab_next_state = [],[]

        for state, action, reward, next_state, done in full_minibatch:
            tab_state.append(state.inputs[0])
        tab_state = np.array(tab_state)

        model_predict_state = self.model.predict(tab_state)

        for i in range(len(full_minibatch)):

            target = full_minibatch[i][2]

            target_f = model_predict_state[i]
            #print(target_f[action])
            target_f[full_minibatch[i][1]] = target


            # Filtering out states and targets for training

            targets_f.append(target_f)

        history = self.model.fit(tab_state, np.array(targets_f), epochs=1, verbose=1)
        # Keeping track of loss
        loss = history.history['loss'][0]
        if self.bot_treshhold > 0.05:
            self.bot_treshhold -= times*0.0000010
        if self.epsilon > self.epsilon_min:
            self.epsilon -= 0.0001

        return loss


    def update_target_model(self):
        # copy weights from model to target_model
        self.target_model.set_weights(self.model.get_weights())

    def load(self, name):
        self.model.load_weights(name)

    def save(self, name):
        self.model.save_weights(name)



EPISODES = 1100

x_total_reward = []
y_total = []

env = Game()

state_size = 13
action_size = env.get_action_size()

agent = DQNAgent(state_size, action_size)
state_game = "start - "
done = False


win = 0
lose = 0
score_stat = 0


TOTAL = 1100



x_1 = []
y_1 = []




e = 0
debut = int(time.time())
end = int(time.time())
#for e in range(EPISODES):
# training for 10 hours : 
while (end - debut) < 3600*10:
    e += 1

    end = int(time.time())
    if (e%20 == 0):
        print("time since start : ",(end-debut))
    env = Game()
    env.generate_random_game()
    env._build_inputs()
    # Batch size for the training : 
    batch_size = 128

    proba_opp = random.random()




    if e%50 == 49:
        print(" #############################################")
        print(" ------- BILAN DES 50 DERNIERES GAMES ------- ")
        x_total_reward.append(score_stat)
        y_total.append(e)

        print("stat_win/lose : {}/{}, score sur ces wins : {}"
                    .format(win,lose, score_stat))
        lose = 0
        win = 0
        score_stat = 0






        print(" ------ DEBUT DES 50 NOUVELLES GAMES ------ ")
        print(" #############################################")

    reward_complete = 0

    pro = random.random()

    agent.proba_opp = proba_opp
    agent.pro = pro

    state_game = ""
    for times in range(1000):

        # env.render()
        env._refresh_inputs()

        action = agent.act(env)
        opp_action = agent.opp_act_train(env,proba_opp)
        if (pro < agent.bot_treshhold):
            tab = env.step_bis(env.possible_actions[action],env.possible_actions[opp_action])
        else:
            tab = env.step(env.possible_actions[action],env.possible_actions[opp_action])

        next_env = tab[1]

        env = tab[0]
        reward = tab[2]
        done = tab[3]
        state = tab[4]
        check = tab[5]
        reset = tab[6]

        state_game += state
        reward_complete += reward
        env._refresh_inputs()
        next_env._refresh_inputs()


        agent.remember(env, action, reward, next_env, done,times,reset)


        env = next_env

        if done or times == 990:

            if (reward > 0):
                win += 1
            if (reward < 0):
                lose += 1
            score_stat += reward_complete

            if e%250 == 0:
                agent.update_target_model()

            print("episode: {}, score: {}, e: {:.6} ||  state at the end : {} in {} turns"
                    .format(e,reward_complete,agent.epsilon,reward,times))

            print("La partie : ",state_game)

            if len(agent.memory) > batch_size:
                loss = agent.replay_full(batch_size,times)
                print("episode: {}/{}, time: {}, loss: {:.6f}"
                    .format(e, EPISODES, times, loss))



            break



print("Agent bot_treshhol :",agent.bot_treshhold)
print("Agent proba_opp :",agent.proba_opp)
print("Agent pro :",agent.pro)
print("Agent p_good :",agent.p_good)






####### Testing our agent, with a basic screen using plt : 






x_1 = []
y_1 = []



state = ""

env = Game()
env.generate_random_game()
env._build_inputs()
batch_size = 32

cp_x = []
cp_y = []

for cp in env.cps:
    cp_x.append(cp.x)
    cp_y.append(cp.y)

for times in range(1000):

        # env.render()
    env._refresh_inputs()

    x_1.append(env.pods[0].x)
    y_1.append(env.pods[0].y)




    action = agent.act(env)
    opp_action = agent.opp_act_train(env,0.1)


    tab = env.step(env.possible_actions[action],env.possible_actions[opp_action])

    next_env = tab[1]
    if times%100 == 99:
        next_env.afficher()

    env = tab[0]
    reward = tab[2]
    done = tab[3]

    state = tab[4]
    state_game += state

    env._refresh_inputs()
    next_env._refresh_inputs()


    agent.remember(env, action, reward, next_env, done,times,False)

    env = next_env

    if done:
        print(state)
        break




x1 = np.array(x_1)
y1 = np.array(y_1)

circle = []

fig, ax = plt.subplots()

ax.set_xlim((0, 16000))
ax.set_ylim((0, 9000))


plt.plot(cp_x,cp_y,'go',linewidth=10.0)

for i in range(len(x_1)):
    plt.scatter(x_1[i],y_1[i],c='red')


    plt.pause(0.05)

plt.show()


x = np.array(x_total_reward)
y = np.array(y_total)

plt.plot(y, x)

plt.xlabel('episodes')
plt.ylabel('total reward on one episode')
plt.title('Evolution of the Q evaluation')
plt.grid(True)
plt.show()
